{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1065a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import\tpandas\tas\tpd\n",
    "import\tnumpy\tas\tnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d64a6bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car</th>\n",
       "      <th>mpg</th>\n",
       "      <th>cyl</th>\n",
       "      <th>disp</th>\n",
       "      <th>hp</th>\n",
       "      <th>drat</th>\n",
       "      <th>wt</th>\n",
       "      <th>qsec</th>\n",
       "      <th>vs</th>\n",
       "      <th>am</th>\n",
       "      <th>gear</th>\n",
       "      <th>carb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mazda RX4</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6</td>\n",
       "      <td>160.0</td>\n",
       "      <td>110</td>\n",
       "      <td>3.90</td>\n",
       "      <td>2.620</td>\n",
       "      <td>16.46</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mazda RX4 Wag</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6</td>\n",
       "      <td>160.0</td>\n",
       "      <td>110</td>\n",
       "      <td>3.90</td>\n",
       "      <td>2.875</td>\n",
       "      <td>17.02</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Datsun 710</td>\n",
       "      <td>22.8</td>\n",
       "      <td>4</td>\n",
       "      <td>108.0</td>\n",
       "      <td>93</td>\n",
       "      <td>3.85</td>\n",
       "      <td>2.320</td>\n",
       "      <td>18.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             car   mpg  cyl   disp   hp  drat     wt   qsec  vs  am  gear  \\\n",
       "0      Mazda RX4  21.0    6  160.0  110  3.90  2.620  16.46   0   1     4   \n",
       "1  Mazda RX4 Wag  21.0    6  160.0  110  3.90  2.875  17.02   0   1     4   \n",
       "2     Datsun 710  22.8    4  108.0   93  3.85  2.320  18.61   1   1     4   \n",
       "\n",
       "   carb  \n",
       "0     4  \n",
       "1     4  \n",
       "2     1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\t데이터 불러오기\tmtcars\n",
    "df\t=\tpd.read_csv(\"mtcars.csv\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa361d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.\tmtcars\t데이터셋의\tmpg열 데이터의 평균이\t20과 같다고 할 수 있는지 검정하시오.\t(유의수준 5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7498b723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. 정규성 검정\n",
    "# 1. 귀무가설 : 20과 같다\n",
    "#    대립가설 : 20과 다르다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75463410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n'ttest_1samp',\\n 'ttest_ind',\\n 'ttest_ind_from_stats',\\n 'ttest_rel',\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.stats\n",
    "\"\"\"\n",
    "'ttest_1samp',\n",
    " 'ttest_ind',\n",
    " 'ttest_ind_from_stats',\n",
    " 'ttest_rel',\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3740de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_1samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e55df76a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function ttest_1samp in module scipy.stats._stats_py:\n",
      "\n",
      "ttest_1samp(a, popmean, axis=0, nan_policy='propagate', alternative='two-sided', *, keepdims=False)\n",
      "    Calculate the T-test for the mean of ONE group of scores.\n",
      "    \n",
      "    This is a test for the null hypothesis that the expected value\n",
      "    (mean) of a sample of independent observations `a` is equal to the given\n",
      "    population mean, `popmean`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    a : array_like\n",
      "        Sample observation.\n",
      "    popmean : float or array_like\n",
      "        Expected value in null hypothesis. If array_like, then its length along\n",
      "        `axis` must equal 1, and it must otherwise be broadcastable with `a`.\n",
      "    axis : int or None, default: 0\n",
      "        If an int, the axis of the input along which to compute the statistic.\n",
      "        The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
      "        corresponding element of the output.\n",
      "        If ``None``, the input will be raveled before computing the statistic.\n",
      "    nan_policy : {'propagate', 'omit', 'raise'}\n",
      "        Defines how to handle input NaNs.\n",
      "        \n",
      "        - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
      "          which the  statistic is computed, the corresponding entry of the output\n",
      "          will be NaN.\n",
      "        - ``omit``: NaNs will be omitted when performing the calculation.\n",
      "          If insufficient data remains in the axis slice along which the\n",
      "          statistic is computed, the corresponding entry of the output will be\n",
      "          NaN.\n",
      "        - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
      "    alternative : {'two-sided', 'less', 'greater'}, optional\n",
      "        Defines the alternative hypothesis.\n",
      "        The following options are available (default is 'two-sided'):\n",
      "        \n",
      "        * 'two-sided': the mean of the underlying distribution of the sample\n",
      "          is different than the given population mean (`popmean`)\n",
      "        * 'less': the mean of the underlying distribution of the sample is\n",
      "          less than the given population mean (`popmean`)\n",
      "        * 'greater': the mean of the underlying distribution of the sample is\n",
      "          greater than the given population mean (`popmean`)\n",
      "    keepdims : bool, default: False\n",
      "        If this is set to True, the axes which are reduced are left\n",
      "        in the result as dimensions with size one. With this option,\n",
      "        the result will broadcast correctly against the input array.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    result : `~scipy.stats._result_classes.TtestResult`\n",
      "        An object with the following attributes:\n",
      "        \n",
      "        statistic : float or array\n",
      "            The t-statistic.\n",
      "        pvalue : float or array\n",
      "            The p-value associated with the given alternative.\n",
      "        df : float or array\n",
      "            The number of degrees of freedom used in calculation of the\n",
      "            t-statistic; this is one less than the size of the sample\n",
      "            (``a.shape[axis]``).\n",
      "        \n",
      "            .. versionadded:: 1.10.0\n",
      "        \n",
      "        The object also has the following method:\n",
      "        \n",
      "        confidence_interval(confidence_level=0.95)\n",
      "            Computes a confidence interval around the population\n",
      "            mean for the given confidence level.\n",
      "            The confidence interval is returned in a ``namedtuple`` with\n",
      "            fields `low` and `high`.\n",
      "        \n",
      "            .. versionadded:: 1.10.0\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    The statistic is calculated as ``(np.mean(a) - popmean)/se``, where\n",
      "    ``se`` is the standard error. Therefore, the statistic will be positive\n",
      "    when the sample mean is greater than the population mean and negative when\n",
      "    the sample mean is less than the population mean.\n",
      "    \n",
      "    Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
      "    code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
      "    this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
      "    rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
      "    arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
      "    masked array with ``mask=False``.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Suppose we wish to test the null hypothesis that the mean of a population\n",
      "    is equal to 0.5. We choose a confidence level of 99%; that is, we will\n",
      "    reject the null hypothesis in favor of the alternative if the p-value is\n",
      "    less than 0.01.\n",
      "    \n",
      "    When testing random variates from the standard uniform distribution, which\n",
      "    has a mean of 0.5, we expect the data to be consistent with the null\n",
      "    hypothesis most of the time.\n",
      "    \n",
      "    >>> import numpy as np\n",
      "    >>> from scipy import stats\n",
      "    >>> rng = np.random.default_rng()\n",
      "    >>> rvs = stats.uniform.rvs(size=50, random_state=rng)\n",
      "    >>> stats.ttest_1samp(rvs, popmean=0.5)\n",
      "    TtestResult(statistic=2.456308468440, pvalue=0.017628209047638, df=49)\n",
      "    \n",
      "    As expected, the p-value of 0.017 is not below our threshold of 0.01, so\n",
      "    we cannot reject the null hypothesis.\n",
      "    \n",
      "    When testing data from the standard *normal* distribution, which has a mean\n",
      "    of 0, we would expect the null hypothesis to be rejected.\n",
      "    \n",
      "    >>> rvs = stats.norm.rvs(size=50, random_state=rng)\n",
      "    >>> stats.ttest_1samp(rvs, popmean=0.5)\n",
      "    TtestResult(statistic=-7.433605518875, pvalue=1.416760157221e-09, df=49)\n",
      "    \n",
      "    Indeed, the p-value is lower than our threshold of 0.01, so we reject the\n",
      "    null hypothesis in favor of the default \"two-sided\" alternative: the mean\n",
      "    of the population is *not* equal to 0.5.\n",
      "    \n",
      "    However, suppose we were to test the null hypothesis against the\n",
      "    one-sided alternative that the mean of the population is *greater* than\n",
      "    0.5. Since the mean of the standard normal is less than 0.5, we would not\n",
      "    expect the null hypothesis to be rejected.\n",
      "    \n",
      "    >>> stats.ttest_1samp(rvs, popmean=0.5, alternative='greater')\n",
      "    TtestResult(statistic=-7.433605518875, pvalue=0.99999999929, df=49)\n",
      "    \n",
      "    Unsurprisingly, with a p-value greater than our threshold, we would not\n",
      "    reject the null hypothesis.\n",
      "    \n",
      "    Note that when working with a confidence level of 99%, a true null\n",
      "    hypothesis will be rejected approximately 1% of the time.\n",
      "    \n",
      "    >>> rvs = stats.uniform.rvs(size=(100, 50), random_state=rng)\n",
      "    >>> res = stats.ttest_1samp(rvs, popmean=0.5, axis=1)\n",
      "    >>> np.sum(res.pvalue < 0.01)\n",
      "    1\n",
      "    \n",
      "    Indeed, even though all 100 samples above were drawn from the standard\n",
      "    uniform distribution, which *does* have a population mean of 0.5, we would\n",
      "    mistakenly reject the null hypothesis for one of them.\n",
      "    \n",
      "    `ttest_1samp` can also compute a confidence interval around the population\n",
      "    mean.\n",
      "    \n",
      "    >>> rvs = stats.norm.rvs(size=50, random_state=rng)\n",
      "    >>> res = stats.ttest_1samp(rvs, popmean=0)\n",
      "    >>> ci = res.confidence_interval(confidence_level=0.95)\n",
      "    >>> ci\n",
      "    ConfidenceInterval(low=-0.3193887540880017, high=0.2898583388980972)\n",
      "    \n",
      "    The bounds of the 95% confidence interval are the\n",
      "    minimum and maximum values of the parameter `popmean` for which the\n",
      "    p-value of the test would be 0.05.\n",
      "    \n",
      "    >>> res = stats.ttest_1samp(rvs, popmean=ci.low)\n",
      "    >>> np.testing.assert_allclose(res.pvalue, 0.05)\n",
      "    >>> res = stats.ttest_1samp(rvs, popmean=ci.high)\n",
      "    >>> np.testing.assert_allclose(res.pvalue, 0.05)\n",
      "    \n",
      "    Under certain assumptions about the population from which a sample\n",
      "    is drawn, the confidence interval with confidence level 95% is expected\n",
      "    to contain the true population mean in 95% of sample replications.\n",
      "    \n",
      "    >>> rvs = stats.norm.rvs(size=(50, 1000), loc=1, random_state=rng)\n",
      "    >>> res = stats.ttest_1samp(rvs, popmean=0)\n",
      "    >>> ci = res.confidence_interval()\n",
      "    >>> contains_pop_mean = (ci.low < 1) & (ci.high > 1)\n",
      "    >>> contains_pop_mean.sum()\n",
      "    953\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ttest_1samp)\n",
    "# (관측, 가설, alternative='two-sided')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc9df99e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ConstantInputWarning',\n",
       " 'Covariance',\n",
       " 'DegenerateDataWarning',\n",
       " 'FitError',\n",
       " 'NearConstantInputWarning',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_axis_nan_policy',\n",
       " '_biasedurn',\n",
       " '_binned_statistic',\n",
       " '_binomtest',\n",
       " '_boost',\n",
       " '_common',\n",
       " '_constants',\n",
       " '_continuous_distns',\n",
       " '_covariance',\n",
       " '_crosstab',\n",
       " '_discrete_distns',\n",
       " '_distn_infrastructure',\n",
       " '_distr_params',\n",
       " '_entropy',\n",
       " '_fit',\n",
       " '_hypotests',\n",
       " '_kde',\n",
       " '_ksstats',\n",
       " '_levy_stable',\n",
       " '_mannwhitneyu',\n",
       " '_morestats',\n",
       " '_mstats_basic',\n",
       " '_mstats_extras',\n",
       " '_multivariate',\n",
       " '_mvn',\n",
       " '_odds_ratio',\n",
       " '_page_trend_test',\n",
       " '_qmc',\n",
       " '_qmc_cy',\n",
       " '_rcont',\n",
       " '_relative_risk',\n",
       " '_resampling',\n",
       " '_rvs_sampling',\n",
       " '_sobol',\n",
       " '_statlib',\n",
       " '_stats',\n",
       " '_stats_mstats_common',\n",
       " '_stats_py',\n",
       " '_stats_pythran',\n",
       " '_tukeylambda_stats',\n",
       " '_variation',\n",
       " '_warnings_errors',\n",
       " 'alexandergovern',\n",
       " 'alpha',\n",
       " 'anderson',\n",
       " 'anderson_ksamp',\n",
       " 'anglit',\n",
       " 'ansari',\n",
       " 'arcsine',\n",
       " 'argus',\n",
       " 'barnard_exact',\n",
       " 'bartlett',\n",
       " 'bayes_mvs',\n",
       " 'bernoulli',\n",
       " 'beta',\n",
       " 'betabinom',\n",
       " 'betaprime',\n",
       " 'biasedurn',\n",
       " 'binned_statistic',\n",
       " 'binned_statistic_2d',\n",
       " 'binned_statistic_dd',\n",
       " 'binom',\n",
       " 'binom_test',\n",
       " 'binomtest',\n",
       " 'boltzmann',\n",
       " 'bootstrap',\n",
       " 'boschloo_exact',\n",
       " 'boxcox',\n",
       " 'boxcox_llf',\n",
       " 'boxcox_normmax',\n",
       " 'boxcox_normplot',\n",
       " 'bradford',\n",
       " 'brunnermunzel',\n",
       " 'burr',\n",
       " 'burr12',\n",
       " 'cauchy',\n",
       " 'chi',\n",
       " 'chi2',\n",
       " 'chi2_contingency',\n",
       " 'chisquare',\n",
       " 'circmean',\n",
       " 'circstd',\n",
       " 'circvar',\n",
       " 'combine_pvalues',\n",
       " 'contingency',\n",
       " 'cosine',\n",
       " 'cramervonmises',\n",
       " 'cramervonmises_2samp',\n",
       " 'crystalball',\n",
       " 'cumfreq',\n",
       " 'describe',\n",
       " 'dgamma',\n",
       " 'differential_entropy',\n",
       " 'directional_stats',\n",
       " 'dirichlet',\n",
       " 'distributions',\n",
       " 'dlaplace',\n",
       " 'dweibull',\n",
       " 'energy_distance',\n",
       " 'entropy',\n",
       " 'epps_singleton_2samp',\n",
       " 'erlang',\n",
       " 'expectile',\n",
       " 'expon',\n",
       " 'exponnorm',\n",
       " 'exponpow',\n",
       " 'exponweib',\n",
       " 'f',\n",
       " 'f_oneway',\n",
       " 'fatiguelife',\n",
       " 'find_repeats',\n",
       " 'fisher_exact',\n",
       " 'fisk',\n",
       " 'fit',\n",
       " 'fligner',\n",
       " 'foldcauchy',\n",
       " 'foldnorm',\n",
       " 'friedmanchisquare',\n",
       " 'gamma',\n",
       " 'gausshyper',\n",
       " 'gaussian_kde',\n",
       " 'genexpon',\n",
       " 'genextreme',\n",
       " 'gengamma',\n",
       " 'genhalflogistic',\n",
       " 'genhyperbolic',\n",
       " 'geninvgauss',\n",
       " 'genlogistic',\n",
       " 'gennorm',\n",
       " 'genpareto',\n",
       " 'geom',\n",
       " 'gibrat',\n",
       " 'gilbrat',\n",
       " 'gmean',\n",
       " 'gompertz',\n",
       " 'goodness_of_fit',\n",
       " 'gstd',\n",
       " 'gumbel_l',\n",
       " 'gumbel_r',\n",
       " 'gzscore',\n",
       " 'halfcauchy',\n",
       " 'halfgennorm',\n",
       " 'halflogistic',\n",
       " 'halfnorm',\n",
       " 'hmean',\n",
       " 'hypergeom',\n",
       " 'hypsecant',\n",
       " 'invgamma',\n",
       " 'invgauss',\n",
       " 'invweibull',\n",
       " 'invwishart',\n",
       " 'iqr',\n",
       " 'jarque_bera',\n",
       " 'johnsonsb',\n",
       " 'johnsonsu',\n",
       " 'kappa3',\n",
       " 'kappa4',\n",
       " 'kde',\n",
       " 'kendalltau',\n",
       " 'kruskal',\n",
       " 'ks_1samp',\n",
       " 'ks_2samp',\n",
       " 'ksone',\n",
       " 'kstat',\n",
       " 'kstatvar',\n",
       " 'kstest',\n",
       " 'kstwo',\n",
       " 'kstwobign',\n",
       " 'kurtosis',\n",
       " 'kurtosistest',\n",
       " 'laplace',\n",
       " 'laplace_asymmetric',\n",
       " 'levene',\n",
       " 'levy',\n",
       " 'levy_l',\n",
       " 'levy_stable',\n",
       " 'linregress',\n",
       " 'loggamma',\n",
       " 'logistic',\n",
       " 'loglaplace',\n",
       " 'lognorm',\n",
       " 'logser',\n",
       " 'loguniform',\n",
       " 'lomax',\n",
       " 'mannwhitneyu',\n",
       " 'matrix_normal',\n",
       " 'maxwell',\n",
       " 'median_abs_deviation',\n",
       " 'median_test',\n",
       " 'mielke',\n",
       " 'mode',\n",
       " 'moment',\n",
       " 'monte_carlo_test',\n",
       " 'mood',\n",
       " 'morestats',\n",
       " 'moyal',\n",
       " 'mstats',\n",
       " 'mstats_basic',\n",
       " 'mstats_extras',\n",
       " 'multinomial',\n",
       " 'multiscale_graphcorr',\n",
       " 'multivariate_hypergeom',\n",
       " 'multivariate_normal',\n",
       " 'multivariate_t',\n",
       " 'mvn',\n",
       " 'mvsdist',\n",
       " 'nakagami',\n",
       " 'nbinom',\n",
       " 'ncf',\n",
       " 'nchypergeom_fisher',\n",
       " 'nchypergeom_wallenius',\n",
       " 'nct',\n",
       " 'ncx2',\n",
       " 'nhypergeom',\n",
       " 'norm',\n",
       " 'normaltest',\n",
       " 'norminvgauss',\n",
       " 'obrientransform',\n",
       " 'ortho_group',\n",
       " 'page_trend_test',\n",
       " 'pareto',\n",
       " 'pearson3',\n",
       " 'pearsonr',\n",
       " 'percentileofscore',\n",
       " 'permutation_test',\n",
       " 'planck',\n",
       " 'pmean',\n",
       " 'pointbiserialr',\n",
       " 'poisson',\n",
       " 'poisson_means_test',\n",
       " 'power_divergence',\n",
       " 'powerlaw',\n",
       " 'powerlognorm',\n",
       " 'powernorm',\n",
       " 'ppcc_max',\n",
       " 'ppcc_plot',\n",
       " 'probplot',\n",
       " 'qmc',\n",
       " 'randint',\n",
       " 'random_correlation',\n",
       " 'random_table',\n",
       " 'rankdata',\n",
       " 'ranksums',\n",
       " 'rayleigh',\n",
       " 'rdist',\n",
       " 'recipinvgauss',\n",
       " 'reciprocal',\n",
       " 'relfreq',\n",
       " 'rice',\n",
       " 'rv_continuous',\n",
       " 'rv_discrete',\n",
       " 'rv_histogram',\n",
       " 'rvs_ratio_uniforms',\n",
       " 'scoreatpercentile',\n",
       " 'sem',\n",
       " 'semicircular',\n",
       " 'shapiro',\n",
       " 'siegelslopes',\n",
       " 'sigmaclip',\n",
       " 'skellam',\n",
       " 'skew',\n",
       " 'skewcauchy',\n",
       " 'skewnorm',\n",
       " 'skewtest',\n",
       " 'somersd',\n",
       " 'spearmanr',\n",
       " 'special_ortho_group',\n",
       " 'statlib',\n",
       " 'stats',\n",
       " 'studentized_range',\n",
       " 't',\n",
       " 'test',\n",
       " 'theilslopes',\n",
       " 'tiecorrect',\n",
       " 'tmax',\n",
       " 'tmean',\n",
       " 'tmin',\n",
       " 'trapezoid',\n",
       " 'trapz',\n",
       " 'triang',\n",
       " 'trim1',\n",
       " 'trim_mean',\n",
       " 'trimboth',\n",
       " 'truncexpon',\n",
       " 'truncnorm',\n",
       " 'truncpareto',\n",
       " 'truncweibull_min',\n",
       " 'tsem',\n",
       " 'tstd',\n",
       " 'ttest_1samp',\n",
       " 'ttest_ind',\n",
       " 'ttest_ind_from_stats',\n",
       " 'ttest_rel',\n",
       " 'tukey_hsd',\n",
       " 'tukeylambda',\n",
       " 'tvar',\n",
       " 'uniform',\n",
       " 'uniform_direction',\n",
       " 'unitary_group',\n",
       " 'variation',\n",
       " 'vonmises',\n",
       " 'vonmises_line',\n",
       " 'wald',\n",
       " 'wasserstein_distance',\n",
       " 'weibull_max',\n",
       " 'weibull_min',\n",
       " 'weightedtau',\n",
       " 'wilcoxon',\n",
       " 'wishart',\n",
       " 'wrapcauchy',\n",
       " 'yeojohnson',\n",
       " 'yeojohnson_llf',\n",
       " 'yeojohnson_normmax',\n",
       " 'yeojohnson_normplot',\n",
       " 'yulesimon',\n",
       " 'zipf',\n",
       " 'zipfian',\n",
       " 'zmap',\n",
       " 'zscore']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(scipy.stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0644cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cebdfdc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.090625000000003"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['mpg'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "095bdabb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TtestResult(statistic=0.08506003568133688, pvalue=0.9327606409093872, df=31)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttest_1samp(df['mpg'], 20, alternative='two-sided') # 같다고 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acfd1e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shapiro\n",
    "from scipy.stats import shapiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a9ff1b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShapiroResult(statistic=0.9475648403167725, pvalue=0.1228824257850647)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapiro(df['mpg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fa7928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9b6853",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90f0d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.\tmtcars\t데이터셋의\tmpg열 데이터의 평균이\t17보다 크다고 할 수 있는지 검정하시오.(유의수\n",
    "# 준\t5%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b43b841d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro\n",
    "from scipy.stats import ttest_1samp, wilcoxon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7135837b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShapiroResult(statistic=0.9475648403167725, pvalue=0.1228824257850647)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapiro(df['mpg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0dca4f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TtestResult(statistic=2.900840527201366, pvalue=0.003394155007833913, df=31)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttest_1samp(df['mpg'], 17, alternative='greater')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea555982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WilcoxonResult(statistic=395.5, pvalue=0.00661516422405839)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wilcoxon(df['mpg']-17 , alternative='greater')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7ecc2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12b1f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.\tmtcars\t데이터셋의\tmpg열 데이터의 평균이\t17보다 작다고 할 수 있는지 검정하시오.(유의수\n",
    "# 준\t5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16de2fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 유의수준 5%\n",
    "# 2. 귀무가설 : 크거나 같다 \n",
    "# 3. 정규성 검정 wil, ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a5ec1872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShapiroResult(statistic=0.9475648403167725, pvalue=0.1228824257850647)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapiro(df['mpg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "394a04d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TtestResult(statistic=2.900840527201366, pvalue=0.9966058449921661, df=31)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttest_1samp(df['mpg'], 17, alternative='less')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b3ec583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WilcoxonResult(statistic=395.5, pvalue=0.9937504890840501)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wilcoxon(df['mpg']-17, alternative='less')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76edee4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d92b31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cdb7c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd7f4f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f8d6e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9905656d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
